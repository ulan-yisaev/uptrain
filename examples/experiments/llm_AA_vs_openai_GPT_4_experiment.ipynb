{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c4a5dc2b9212820d",
   "metadata": {},
   "source": [
    "<h1 align=\"center\">\n",
    "  <a href=\"https://www.nlga.niedersachsen.de/startseite\">\n",
    "    <img width=\"300\" src=\"https://www.nlga.niedersachsen.de/assets/image/246974\" alt=\"NLGA\">\n",
    "  </a>\n",
    "</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c529441-0282-43e2-a443-9cff01b2c64e",
   "metadata": {},
   "source": [
    "## Experimenting with different LLMs - Aleph Alpha Luminous-supreme-control vs GPT-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "548b3a0770d1916c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-18T20:16:37.878452Z",
     "start_time": "2024-04-18T20:16:37.867516Z"
    }
   },
   "outputs": [],
   "source": [
    "# define configuration for dynamic experiment handling\n",
    "EXPERIMENT_NAME = \"GPT_4_vs_Luminous_Supreme\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6370d692-f235-46fb-a72c-7a6da2e89cb8",
   "metadata": {},
   "source": [
    "**Overview**: In this notebook, we will compare different LLM providers. We will be using around 35 example questions from the [Testfragen](https://secure-confluence.nortal.com/display/NLGAC/Testfragen) dataset and evaluate the response on different criteria to determine which of the two models performs better.\n",
    "\n",
    "We have used the following metrics from UpTrain's library:\n",
    "\n",
    "1. [Response Conciseness](https://docs.uptrain.ai/predefined-evaluations/response-quality/response-conciseness): Evaluates how concise the generated response is or if it has any additional irrelevant information for the question asked.\n",
    "\n",
    "2. [Response Matching](https://docs.uptrain.ai/predefined-evaluations/ground-truth-comparison/response-matching): Evaluates how well the response generated by the LLM aligns with the provided ground truth.\n",
    "\n",
    "3. [Factual Accuracy](https://docs.uptrain.ai/predefined-evaluations/context-awareness/factual-accuracy): Evaluates whether the response generated is factually correct and grounded by the provided context.\n",
    "\n",
    "4. [Context Utilization](https://docs.uptrain.ai/predefined-evaluations/context-awareness/context-utilization): Evaluates how complete the generated response is for the question specified given the information provided in the context. Also known as Reponse Completeness wrt context (RESPONSE_COMPLETENESS_WRT_CONTEXT)\n",
    "\n",
    "5. [Response Relevance](https://docs.uptrain.ai/predefined-evaluations/response-quality/response-relevance): Evaluates how relevant the generated response was to the question specified.\n",
    "6. **What is Response Validity?**: In some cases, an LLM might fail to generate a response due to reasons like limited knowledge or the asked question not being clear. Response Validity score can be used to identify these cases, where a model is not generating an informative response.\n",
    "For example, if the question asked is \"What is the chemical formula of chlorophyll?\", a valid response would be \"The  formula for chlorophyll is C55H72O5N4Mg.\" An invalid response could be \"Sorry, I have no idea about that.\"\n",
    "7. **What is Guideline Adherence?**: [Guideline adherence](https://github.com/uptrain-ai/uptrain/blob/main/examples/checks/custom/guideline_adherence.ipynb) refers to the extent to which the LLM follows a given guideline, rule, or protocol. Given the complexity of LLMs, it is crucial to define certain guidelines, be it in terms of the structure of the output or the constraints on the content of the output or protocols on the decision-making capabilities of the LLM (agents). \n",
    "For example, for an LLM-powered chatbot agent trained to perform appointment booking tasks only, you want to make sure that the LLM is following the guideline: \"The agent should redirect all the queries to the human agent, except the ones related to appointment booking.\"\n",
    "\n",
    "Each score has a value between 0 and 1. \n",
    "\n",
    "Complete list of UpTrain's supported metrics [here](https://docs.uptrain.ai/predefined-evaluations/overview)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "543bb6c2",
   "metadata": {},
   "source": [
    "### Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c876701-7a13-4b71-8164-02e0c4849e22",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-18T19:59:18.289370Z",
     "start_time": "2024-04-18T19:59:18.286103Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# %pip install openai uptrain together lazy_loader fsspec pandas polars networkx pydantic_settings aiolimiter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c6a6872653287b5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-18T20:16:30.827009Z",
     "start_time": "2024-04-18T20:16:29.484131Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import polars as pl \n",
    "import shutil\n",
    "from datetime import datetime\n",
    "import time\n",
    "from openai import OpenAI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9891cb41-2d33-4e6e-b16d-7330a6d52326",
   "metadata": {},
   "source": [
    "### Authentication and Configuration \n",
    "\n",
    "Let's define the required api keys - mainly the Azure openai key (for generating responses).\n",
    "Please also ensure that the dataset path is correctly defined in the configuration.\n",
    "Do not forget to set the API_KEY and BASE_URL for the LLM API Endpoint provider. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef2dda84-13bb-4fcc-a7b6-0d6efd22ba41",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-18T20:16:40.671985Z",
     "start_time": "2024-04-18T20:16:40.656619Z"
    }
   },
   "outputs": [],
   "source": [
    "# Load the environment variables from the .env file\n",
    "load_dotenv()\n",
    "\n",
    "CONFIG = {\n",
    "    # The model name used to generate responses\n",
    "    \"GENERATE_MODEL_NAME\": \"gpt4\",\n",
    "    \"AA_MODEL_NAME\": \"luminous-supreme-control\",\n",
    "    # Guideline name used in the Guideline Adherence check \n",
    "    \"GUIDELINE_NAME\": \"Strict_Context\",\n",
    "    # dataset path\n",
    "    # \"DATASET_PATH\": \"nlga_dataset_AA_small.jsonl\",\n",
    "    \"DATASET_PATH\": \"./nlga_dataset_AA.jsonl\",\n",
    "    \"RESULTS_DIR\": \"./results/\",\n",
    "    \"AZURE_OPENAI_API_KEY\": os.getenv(\"AZURE_OPENAI_API_KEY\"),\n",
    "    \"AZURE_API_VERSION\": os.getenv(\"AZURE_API_VERSION\"),\n",
    "    \"AZURE_API_BASE\": os.getenv(\"AZURE_API_BASE\"),\n",
    "    # Azure deployments:\n",
    "    \"GPT_35_TURBO_16K\": \"gpt-35-turbo-16k-deployment\",\n",
    "    \"GPT_4\": \"gpt4\"\n",
    "}\n",
    "\n",
    "# Azure deployment used to evaluate \n",
    "EVAL_MODEL_NAME = \"azure/gpt4\"\n",
    "# EVAL_MODEL_NAME = \"azure/gpt35-16k\"\n",
    "\n",
    "def get_experiment_file_path(extension):\n",
    "    filename = f\"{EXPERIMENT_NAME.replace(' ', '_').replace('-', '_').lower()}_experiment.{extension}\"\n",
    "    return os.path.join(CONFIG['RESULTS_DIR'], filename)\n",
    "\n",
    "jsonl_file_path = get_experiment_file_path('jsonl')\n",
    "csv_file_path = get_experiment_file_path('csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f913017402b725b1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-18T20:16:43.248492Z",
     "start_time": "2024-04-18T20:16:43.232346Z"
    }
   },
   "outputs": [],
   "source": [
    "# Utility functions for API and file operations\n",
    "from openai import AzureOpenAI\n",
    "\n",
    "def initialize_azure_openai_client():\n",
    "    # gets the API Key from environment variable AZURE_OPENAI_API_KEY\n",
    "    return AzureOpenAI(api_version=CONFIG[\"AZURE_API_VERSION\"], \n",
    "                       azure_endpoint=CONFIG[\"AZURE_API_BASE\"])\n",
    "\n",
    "def ensure_directory_exists(path):\n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(path)\n",
    "\n",
    "def read_dataset(path):\n",
    "    if not os.path.exists(path):\n",
    "        raise FileNotFoundError(f\"The specified dataset path does not exist: {path}\")\n",
    "    return pl.read_ndjson(path)\n",
    "# dataset = pl.read_ndjson(dataset_path).select(pl.col([\"question\", \"ground_truth\", \"context\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a3717e1-3752-4a7a-a746-a151b32b06dd",
   "metadata": {},
   "source": [
    "### Load the testing dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c2da51d-1f13-4a77-bd95-d2ebf75afb59",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-18T19:59:19.295562Z",
     "start_time": "2024-04-18T19:59:19.277870Z"
    }
   },
   "outputs": [],
   "source": [
    "ensure_directory_exists(CONFIG['RESULTS_DIR'])\n",
    "dataset = read_dataset(CONFIG['DATASET_PATH'])\n",
    "filtered_dataset = dataset.filter(dataset[\"idx\"] > 100)\n",
    "filtered_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c05cfdf-f5c5-4217-846f-65beaba32118",
   "metadata": {},
   "source": [
    "### Let's define a prompt to generate responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3408803180283c7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-18T19:59:19.301520Z",
     "start_time": "2024-04-18T19:59:19.296568Z"
    }
   },
   "outputs": [],
   "source": [
    "SYSTEM_PROMPT = \"\"\"### INSTRUKTIONEN\n",
    "Generiere bitte eine ANTWORT, die sich strikt an den gegebenen KONTEXT hält und präzise auf die gestellte FRAGE antwortet, ohne eigene Informationen des Modells hinzuzufügen. Falls die benötigte Information nicht im KONTEXT zu finden ist, antworte mit: 'Ihre Anfrage kann nicht mit den bereitgestellten Daten beantwortet werden. Bitte erläutern Sie Ihre Anfrage genauer oder geben Sie weitere Informationen an, falls notwendig.'. Vermeide Bezüge auf vorherige Ausgaben des Modells. Die Antwort soll auf dem bereitgestellten KONTEXT basieren. Sollte die FRAGE nicht direkt einem gesundheitsbezogenen Thema zuzuordnen sein oder nicht klar zu beantworten sein, erkläre kurz, warum die Anfrage nicht beantwortet werden kann und empfehle eine genauere Formulierung oder zusätzliche Informationen.\n",
    "\n",
    "### KONTEXT\n",
    "{context}\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aacb452-37fd-4b5a-9485-829c8fb22644",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-18T19:59:20.257544Z",
     "start_time": "2024-04-18T19:59:19.303538Z"
    }
   },
   "outputs": [],
   "source": [
    "client = initialize_azure_openai_client()\n",
    "\n",
    "def get_response(row, model):\n",
    "    question = row['question'][0]\n",
    "    context = row['context'][0]\n",
    "\n",
    "    if \"gpt\" in model:\n",
    "        response = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": SYSTEM_PROMPT.format(context=context)},\n",
    "            {\"role\": \"user\", \"content\": question},\n",
    "            # {\"role\": \"assistant\", \"content\": \"Example answer\"},\n",
    "            # {\"role\": \"user\", \"content\": \"First question/message for the model to actually respond to.\"}\n",
    "        ]\n",
    "    ).choices[0].message.content\n",
    "    \n",
    "    return {'question': question, 'context': context, 'response': response, 'ground_truth': row['ground_truth'][0], 'model': model}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4778d6370960eb67",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-18T19:59:20.263241Z",
     "start_time": "2024-04-18T19:59:20.258551Z"
    }
   },
   "outputs": [],
   "source": [
    "def format_response(row):\n",
    "    question = row['question'][0]\n",
    "    context = row['context'][0]\n",
    "    response = row['response'][0]\n",
    "    ground_truth = row['ground_truth'][0]\n",
    "    model = CONFIG[\"AA_MODEL_NAME\"]\n",
    "\n",
    "    return {'question': question, 'context': context, 'response': response, 'ground_truth': ground_truth, 'model': model}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a1750f0-d2d6-4ac1-85de-df4f60366f1d",
   "metadata": {},
   "source": [
    "### Generate responses for both the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cca09106-6d51-41cc-9452-7a412f35a8c1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-18T20:04:26.295094Z",
     "start_time": "2024-04-18T19:59:20.264754Z"
    }
   },
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "results_cmodel = [get_response(dataset[idx], CONFIG[\"GENERATE_MODEL_NAME\"]) for idx in range(len(dataset))]\n",
    "results_aleph_alpha = [format_response(dataset[idx]) for idx in range(len(dataset))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a693bf99f36071dc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-18T20:04:26.315684Z",
     "start_time": "2024-04-18T20:04:26.298155Z"
    }
   },
   "outputs": [],
   "source": [
    "print(f\"Total execution time: {time.time() - start_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a8e78bbf6990a80",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-18T20:04:26.359159Z",
     "start_time": "2024-04-18T20:04:26.324994Z"
    }
   },
   "outputs": [],
   "source": [
    "pl.Config.set_fmt_str_lengths(50)\n",
    "df = pl.DataFrame(results_cmodel)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fb3fb53-35a8-4b2d-9f04-ba7550059c67",
   "metadata": {},
   "source": [
    "### Evaluating Experiments using UpTrain\n",
    "\n",
    "UpTrain's EvalLLM provides an `evaluate_experiments` method which takes the input data to be evaluated along with the list of checks to be run and the name of the columns associated with the experiment. The method returns a list of dictionaries containing the results of the evaluation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b400ed9a-7c7e-49a9-bbe8-c57ee84e5f06",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-18T20:13:05.718972Z",
     "start_time": "2024-04-18T20:04:26.359159Z"
    }
   },
   "outputs": [],
   "source": [
    "from uptrain import EvalLLM, Evals, ResponseMatching, Settings\n",
    "\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "\n",
    "settings = Settings(model=EVAL_MODEL_NAME, azure_api_key=CONFIG[\"AZURE_OPENAI_API_KEY\"], azure_api_version=CONFIG[\"AZURE_API_VERSION\"], azure_api_base=CONFIG[\"AZURE_API_BASE\"])\n",
    "eval_llm = EvalLLM(settings)\n",
    "\n",
    "res = eval_llm.evaluate_experiments(\n",
    "    project_name = f\"{EXPERIMENT_NAME}-Experiments\",\n",
    "    data = results_cmodel + results_aleph_alpha,\n",
    "    checks = [\n",
    "       Evals.RESPONSE_CONCISENESS,\n",
    "       ResponseMatching(method='llm'),  # Comment this if you don't have Ground Truth\n",
    "       Evals.RESPONSE_COMPLETENESS_WRT_CONTEXT,\n",
    "       Evals.FACTUAL_ACCURACY,\n",
    "       Evals.RESPONSE_RELEVANCE,\n",
    "        Evals.VALID_RESPONSE\n",
    "    ],\n",
    "    exp_columns=['model']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47ea8433d60ffd67",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-18T20:13:05.738664Z",
     "start_time": "2024-04-18T20:13:05.724678Z"
    }
   },
   "outputs": [],
   "source": [
    "print(f\"Total evaluation time: {time.time() - start_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de1fdba0dd3ebf9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-18T20:13:05.791363Z",
     "start_time": "2024-04-18T20:13:05.742211Z"
    }
   },
   "outputs": [],
   "source": [
    "res_df = pl.DataFrame(res)\n",
    "res_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "423984bb6c983272",
   "metadata": {},
   "source": [
    "### Adding Guideline Adherence evaluations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbbf6e03966e08aa",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-18T20:13:05.796736Z",
     "start_time": "2024-04-18T20:13:05.791363Z"
    }
   },
   "outputs": [],
   "source": [
    "guideline = \"The response must strictly adhere to the provided context and not introduce external information. If the necessary information is absent from the context, respond with: 'Ihre Anfrage kann nicht mit den bereitgestellten Daten beantwortet werden. Bitte erläutern Sie Ihre Anfrage genauer oder geben Sie weitere Informationen an, falls notwendig.'. Should the question fall outside the health-related jurisdiction of the Landesgesundheitsamt Niedersachsen, it means the query is beyond the health-related scope and shouldn't be answered.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a712e3208caf97da",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-18T20:13:05.802874Z",
     "start_time": "2024-04-18T20:13:05.796925Z"
    }
   },
   "outputs": [],
   "source": [
    "data_cmodel_for_guideline_eval = [{'question': i['question'], 'response': i['response']} for i in results_cmodel]\n",
    "data_aleph_alpha_for_guideline_eval = [{'question': i['question'], 'response': i['response']} for i in results_aleph_alpha]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14967ae285878fa6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-18T20:13:43.556338Z",
     "start_time": "2024-04-18T20:13:05.802874Z"
    }
   },
   "outputs": [],
   "source": [
    "from uptrain import GuidelineAdherence\n",
    "\n",
    "def run_guideline_adherence_eval(data, guideline_name):\n",
    "    return eval_llm.evaluate(\n",
    "        data=data,\n",
    "        checks=[GuidelineAdherence(guideline=guideline, guideline_name=guideline_name)]\n",
    "    )\n",
    "\n",
    "res_guideline_cmodel = run_guideline_adherence_eval(data_cmodel_for_guideline_eval, CONFIG[\"GUIDELINE_NAME\"])\n",
    "res_guideline_aleph_alpha = run_guideline_adherence_eval(data_aleph_alpha_for_guideline_eval, CONFIG[\"GUIDELINE_NAME\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95bdfe637c6d67cb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-18T20:13:43.580832Z",
     "start_time": "2024-04-18T20:13:43.559362Z"
    }
   },
   "outputs": [],
   "source": [
    "def update_guidelines(guideline_name, res_guidelines, config_model_name):\n",
    "    DEFAULT_SCORE = float(\"nan\")\n",
    "    DEFAULT_EXPLANATION = \"No data available\"\n",
    "    score_name = 'score_' + guideline_name + '_adherence'\n",
    "    explanation_name = 'explanation_' + guideline_name + '_adherence'\n",
    "    \n",
    "    for f in res_guidelines:\n",
    "        score_key = score_name + '_model_' + config_model_name\n",
    "        explanation_key = explanation_name + '_model_' + config_model_name\n",
    "        \n",
    "        if score_name in f:\n",
    "            f[score_key] = f.pop(score_name)\n",
    "        else:\n",
    "            f[score_key] = DEFAULT_SCORE\n",
    "        \n",
    "        if explanation_name in f:\n",
    "            f[explanation_key] = f.pop(explanation_name)\n",
    "        else:\n",
    "            if score_key not in f or f[score_key] == DEFAULT_SCORE:\n",
    "                f[explanation_key] = DEFAULT_EXPLANATION\n",
    "\n",
    "    return res_guidelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d74b92b82f790a5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-18T20:13:43.622657Z",
     "start_time": "2024-04-18T20:13:43.587477Z"
    }
   },
   "outputs": [],
   "source": [
    "res_guideline_cmodel = update_guidelines(CONFIG[\"GUIDELINE_NAME\"], res_guideline_cmodel, CONFIG[\"GENERATE_MODEL_NAME\"])\n",
    "res_guideline_aleph_alpha = update_guidelines(CONFIG[\"GUIDELINE_NAME\"], res_guideline_aleph_alpha, CONFIG[\"AA_MODEL_NAME\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94ba0073d00e6819",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-18T20:13:43.645227Z",
     "start_time": "2024-04-18T20:13:43.622657Z"
    }
   },
   "outputs": [],
   "source": [
    "def merge_lists(base_list, update_list):\n",
    "    update_dict = {item['question']: item for item in update_list if 'question' in item}\n",
    "    \n",
    "    for item in base_list:\n",
    "        question = item.get('question')\n",
    "        if question and question in update_dict:\n",
    "            # print(f\"updating with {question}\")            \n",
    "            update_info = {key: val for key, val in update_dict[question].items() if key != 'response'}\n",
    "            item.update(update_info)\n",
    "    return base_list\n",
    "\n",
    "res=merge_lists(res, res_guideline_cmodel)\n",
    "res=merge_lists(res, res_guideline_aleph_alpha)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94ee7cea5730dff",
   "metadata": {},
   "source": [
    "### Creating Dataframe and displaying Average Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96ed64981b6b7eb2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-18T20:13:43.717150Z",
     "start_time": "2024-04-18T20:13:43.645227Z"
    }
   },
   "outputs": [],
   "source": [
    "res_df = pl.DataFrame(res)\n",
    "res_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6af9f39d46c5814",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-18T20:13:43.780122Z",
     "start_time": "2024-04-18T20:13:43.717150Z"
    }
   },
   "outputs": [],
   "source": [
    "def backup_and_save_df(df, file_path, file_type='csv'):\n",
    "    backup_dir = os.path.join(os.path.dirname(file_path), 'backups')\n",
    "    if not os.path.exists(backup_dir):\n",
    "        os.makedirs(backup_dir)\n",
    "    \n",
    "    if os.path.exists(file_path):\n",
    "        timestamp = datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "        backup_filename = os.path.basename(file_path) + f\".backup-{timestamp}\"\n",
    "        backup_path = os.path.join(backup_dir, backup_filename)\n",
    "        shutil.copy(file_path, backup_path)\n",
    "    \n",
    "    if file_type == 'csv':\n",
    "        print(f\"Saving DataFrame to CSV at: {file_path}\")\n",
    "        df.write_csv(file_path)\n",
    "    elif file_type == 'jsonl':\n",
    "        print(f\"Saving DataFrame to NDJSON at: {file_path}\")\n",
    "        df.write_ndjson(file_path)\n",
    "    \n",
    "backup_and_save_df(res_df, jsonl_file_path, 'jsonl')\n",
    "backup_and_save_df(res_df, csv_file_path, 'csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94ed33b7597ec059",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-18T20:16:53.323283Z",
     "start_time": "2024-04-18T20:16:53.301969Z"
    }
   },
   "outputs": [],
   "source": [
    "def display_average_scores(df):\n",
    "    score_columns = [col for col in df.columns if 'score' in col]\n",
    "    data_for_table = []\n",
    "    \n",
    "    for column in score_columns:\n",
    "        average = df[column].drop_nans().mean()\n",
    "        \n",
    "        parts = column.split('_model_')\n",
    "        # print(f\"___ parts: {parts}\")\n",
    "        metric_name = parts[0].replace('score_', '').replace('_', ' ').capitalize()\n",
    "        model_name = parts[1]\n",
    "        # print(f\"metric_name: {metric_name}, average: {average}\")\n",
    "        # print(f\"model_name: {model_name}\")\n",
    "        \n",
    "        data_for_table.append({\n",
    "            \"Model\": model_name,\n",
    "            \"Metric\": metric_name,\n",
    "            \"Average Score\": average\n",
    "        })\n",
    "    \n",
    "    results_table = pl.DataFrame(data_for_table)\n",
    "    # print(data_for_table)\n",
    "    return results_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d92cfe0ea7c63ba0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-18T20:13:43.839596Z",
     "start_time": "2024-04-18T20:13:43.803813Z"
    }
   },
   "outputs": [],
   "source": [
    "pl.Config.set_tbl_rows(32)\n",
    "pl.Config.set_fmt_str_lengths(50)\n",
    "display_average_scores(res_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34be2b37b19a40b8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-18T20:16:56.598186Z",
     "start_time": "2024-04-18T20:16:56.527277Z"
    }
   },
   "outputs": [],
   "source": [
    "_res_df = read_dataset(jsonl_file_path)\n",
    "pl.Config.set_tbl_rows(32)\n",
    "pl.Config.set_fmt_str_lengths(50)\n",
    "display_average_scores(_res_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2e72b46912d9b74",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-18T20:13:43.854635Z",
     "start_time": "2024-04-18T20:13:43.843134Z"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
